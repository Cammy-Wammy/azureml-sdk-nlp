{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #1: Train an text classification model with Azure Machine Learning\n",
    "\n",
    "In this tutorial, you train a machine learning model both locally and on remote compute resources. You'll use the training and deployment workflow for Azure Machine Learning service (preview) in a Python Jupyter notebook.  You can then use the notebook as a template to train your own machine learning model with your own data. This tutorial is **part one of a two-part tutorial series**.  \n",
    "\n",
    "////\n",
    "This tutorial trains a simple logistic regression using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and [scikit-learn](http://scikit-learn.org) with Azure Machine Learning.  MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of 28x28 pixels, representing a number from 0 to 9. The goal is to create a multi-class classifier to identify the digit a given image represents. \n",
    "////\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Access and examine the data\n",
    "> * Train a simple logistic regression model locally using the popular scikit-learn machine learning library \n",
    "> * Train multiple models on a remote cluster\n",
    "> * Review training results, find and register the best model\n",
    "\n",
    "You'll learn how to select a model and deploy it in [part two of this tutorial](deploy-models.ipynb) later. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Use [these instructions](https://aka.ms/aml-how-to-configure-environment) to:  \n",
    "* Create a workspace and its configuration file (**config.json**)  \n",
    "* Save your **config.json** to the same folder as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/erimen_cse/AMLProjects/MachineLearningNotebooks/tutorials/config.json\n",
      "azurenlp\twestus2\tmshsharedrg\twestus2\n"
     ]
    }
   ],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'keras-reuters'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-01-21T02:00:28.714000+00:00', 'creationTime': '2019-01-21T01:59:40.500222+00:00', 'currentNodeCount': 0, 'errors': None, 'modifiedTime': '2019-01-21T02:00:36.568963+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 0, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 0, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "# vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the necessary packages and compute resources to train a model in the cloud. \n",
    "\n",
    "## Explore data\n",
    "\n",
    "Before you train a model, you need to understand the data that you are using to train it.  You also need to copy the data into the cloud so it can be accessed by your cloud training environment.  In this section you learn how to:\n",
    "\n",
    "* Download the Reuters dataset\n",
    "* Display some sample news text\n",
    "* Upload data to the cloud\n",
    "\n",
    "### Download the Reuters dataset\n",
    "\n",
    "Download the MNIST dataset and save the files into a `data` directory locally.  Images and labels for both training and testing are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data-reuters/reuters_word_index.json',\n",
       " <http.client.HTTPMessage at 0x7fa2b4ac38d0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "os.makedirs('./data-reuters', exist_ok = True)\n",
    "\n",
    "urllib.request.urlretrieve('https://s3.amazonaws.com/text-datasets/reuters.npz', filename='./data-reuters/reuters.npz')\n",
    "urllib.request.urlretrieve('https://s3.amazonaws.com/text-datasets/reuters_word_index.json', filename='./data-reuters/reuters_word_index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have an idea of what these images look like and the expected prediction outcome.\n",
    "\n",
    "### Upload data to the cloud\n",
    "\n",
    "Now make the data accessible remotely by uploading that data from your local machine into Azure so it can be accessed for remote training. The datastore is a convenient construct associated with your workspace for you to upload/download data, and interact with it from your remote compute targets. It is backed by Azure blob storage account.\n",
    "\n",
    "The MNIST files are uploaded into a directory named `mnist` at the root of the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob azurenlp7906480779 azureml-blobstore-f0b90d41-6629-4f43-a97d-d06934eb98ca\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_7e91fdf9959e4d4b89d8ea8435343120"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "ds.upload(src_dir='./data-reuters', target_path='reuters', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have everything you need to start training a model. \n",
    "\n",
    "## Train a local model\n",
    "\n",
    "Train a simple logistic regression model using scikit-learn locally.\n",
    "\n",
    "**Training locally can take a minute or two** depending on your computer configuration.\n",
    "\n",
    "https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training Samples: 8982\n",
      "# of Test Samples: 2246\n",
      "# of Classes: 46\n",
      "the mths always march prerecorded interest attend that in simon shares a in season secretary simon better envisaged liquidation 3 could added mln a for shares further last in below mln 120 march crowley congressional make a cash barred and overnight that japan 124 lme direct and competing in talks proposal a one per important for withheld subsidiaries general 3 that although 1 108 by in shares states will provisionally 1p liquidation mln in 8 mln in political output as owners 50 vanadium system assets closing 3 by day mill ec next 3 in review 1990s novel over bhd miles a in offer pct dlrs\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print('# of Training Samples: {}'.format(len(x_train)))\n",
    "print('# of Test Samples: {}'.format(len(x_test)))\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "print('# of Classes: {}'.format(num_classes))\n",
    "# of Training Samples: 8982\n",
    "# of Test Samples: 2246\n",
    "# of Classes: 46\n",
    "index_to_word = {}\n",
    "for key, value in word_index.items():\n",
    "    index_to_word[value] = key\n",
    "print(' '.join([index_to_word[x] for x in x_train[8981]]))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in word_index.items():\n",
    "    print (key)\n",
    "    print (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "unique, counts = numpy.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  55,  432,   74, 3159, 1949,   17,   48,   16,  139,  101,  124,\n",
       "        390,   49,  172,   26,   20,  444,   39,   66,  549,  269,  100,\n",
       "         15,   41,   62,   92,   24,   15,   48,   19,   45,   39,   32,\n",
       "         11,   50,   10,   49,   19,   19,   24,   36,   30,   13,   21,\n",
       "         12,   18])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "10000\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train[0])\n",
    "print(len(x_train[0]))\n",
    "\n",
    "print(y_train[0])\n",
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/3\n",
      "8083/8083 [==============================] - 32s 4ms/step - loss: 1.3081 - acc: 0.7159 - val_loss: 0.9678 - val_acc: 0.7976\n",
      "Epoch 2/3\n",
      "8083/8083 [==============================] - 2s 291us/step - loss: 0.5143 - acc: 0.8853 - val_loss: 0.8791 - val_acc: 0.8165\n",
      "Epoch 3/3\n",
      "8083/8083 [==============================] - 2s 287us/step - loss: 0.2850 - acc: 0.9364 - val_loss: 0.9065 - val_acc: 0.8076\n",
      "2246/2246 [==============================] - 0s 76us/step\n",
      "Test loss: 0.8825503981548659\n",
      "Test accuracy: 0.797417631397689\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.metrics_names)\n",
    "['loss', 'acc']\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7975528364849833, 0.8164627364400496, 0.8075639600218072]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a few lines of code, you have a 92% accuracy.\n",
    "\n",
    "## Train on a remote cluster\n",
    "\n",
    "Now you can expand on this simple model by building a model with a different regularization rate. This time you'll train the model on a remote resource.  \n",
    "\n",
    "For this task, submit the job to the remote training cluster you set up earlier.  To submit a job you:\n",
    "* Create a directory\n",
    "* Create a training script\n",
    "* Create an estimator object\n",
    "* Submit the job \n",
    "\n",
    "### Create a directory\n",
    "\n",
    "Create a directory to deliver the necessary code from your computer to the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './keras-reuters'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. This training adds a regularization rate to the training algorithm, so produces a slightly different model than the local version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./keras-reuters/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "from azureml.core import Run\n",
    "# from utils import load_data\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--batch-size', type=int, dest='bs', default=32, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=3, help='num of epochs')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = os.path.join(args.data_folder, 'reuters')\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "# X_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\n",
    "# X_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n",
    "# y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\n",
    "# y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path = os.path.join(data_folder, 'reuters.npz'),\n",
    "                                                         num_words=None, \n",
    "                                                         test_split=0.2)\n",
    "word_index = reuters.get_word_index(path=os.path.join(data_folder,\"reuters_word_index.json\"))\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, sep = '\\n')\n",
    "num_classes = max(y_train) + 1\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# print('Train a logistic regression model with regularizaion rate of', args.reg)\n",
    "# clf = LogisticRegression(C=1.0/args.reg, random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "print ('Train a classification model')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.metrics_names)\n",
    "['loss', 'acc']\n",
    "# batch_size = 32\n",
    "# epochs = 3\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=args.bs, epochs=args.epochs, verbose=1, validation_split=0.1)\n",
    "run.log('validation_acc', np.float(history.history['val_acc'][-1]))\n",
    "\n",
    "# print('Predict the test set')\n",
    "# y_hat = clf.predict(X_test)\n",
    "\n",
    "print('Predict the test set')\n",
    "score = model.evaluate(x_test, y_test, batch_size=args.bs, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# # calculate accuracy on the prediction\n",
    "# acc = np.average(y_hat == y_test)\n",
    "# print('Accuracy is', acc)\n",
    "\n",
    "# run.log('regularization rate', np.float(args.reg))\n",
    "# run.log('accuracy', np.float(acc))\n",
    "\n",
    "run.log('batch size', np.int32(args.bs))\n",
    "run.log('epochs', np.int32(args.epochs))\n",
    "run.log('accuracy', np.float(score[1]))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "#joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')\n",
    "model.save('outputs/reuters_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator\n",
    "\n",
    "An estimator object is used to submit the run.  Create your estimator by running the following code to define:\n",
    "\n",
    "* The name of the estimator object, `est`\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the AmlCompute you created\n",
    "* The training script name, train.py\n",
    "* Parameters required from the training script \n",
    "* Python packages needed for training\n",
    "\n",
    "In this tutorial, this target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the datastore (`ds.as_mount()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--batch-size': 32,\n",
    "    '--epochs' : 5\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                use_docker = True,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['keras', 'tensorflow-gpu'],\n",
    "                pip_packages = ['azure-cli-core<2.0.55'],\n",
    "                use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "Run the experiment by submitting the estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>keras-reuters</td><td>keras-reuters_1548056330117</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/e5dac1c0-08c6-4b15-b774-266557580cd9/resourceGroups/mshsharedrg/providers/Microsoft.MachineLearningServices/workspaces/azurenlp/experiments/keras-reuters/runs/keras-reuters_1548056330117\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: keras-reuters,\n",
       "Id: keras-reuters_1548056330117,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the script dependencies don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is uploaded to the workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a83b66cc96042dd861fe75225b67b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion\n",
    "\n",
    "Model training and monitoring happen in the background. Wait until the model has completed training before running more code. Use `wait_for_completion` to show when the model training is complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'keras-reuters_1548056330117',\n",
       " 'target': 'gpucluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2019-01-21T07:47:29.153386Z',\n",
       " 'endTimeUtc': '2019-01-21T07:51:21.866915Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': '1f740b9f-fa4b-4213-9c66-71ce014e33b8'},\n",
       " 'runDefinition': {'Script': 'train.py',\n",
       "  'Arguments': ['--data-folder',\n",
       "   '$AZUREML_DATAREFERENCE_workspaceblobstore',\n",
       "   '--batch-size',\n",
       "   '32',\n",
       "   '--epochs',\n",
       "   '5'],\n",
       "  'SourceDirectoryDataStore': None,\n",
       "  'Framework': 0,\n",
       "  'Communicator': 0,\n",
       "  'Target': 'gpucluster',\n",
       "  'DataReferences': {'workspaceblobstore': {'DataStoreName': 'workspaceblobstore',\n",
       "    'Mode': 'Mount',\n",
       "    'PathOnDataStore': None,\n",
       "    'PathOnCompute': None,\n",
       "    'Overwrite': False}},\n",
       "  'JobName': None,\n",
       "  'AutoPrepareEnvironment': True,\n",
       "  'MaxRunDurationSeconds': None,\n",
       "  'NodeCount': 1,\n",
       "  'Environment': {'Python': {'InterpreterPath': 'python',\n",
       "    'UserManagedDependencies': False,\n",
       "    'CondaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults', 'azure-cli-core<2.0.55']},\n",
       "      'keras',\n",
       "      'tensorflow-gpu']}},\n",
       "   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE',\n",
       "    'NCCL_SOCKET_IFNAME': '^docker0'},\n",
       "   'Docker': {'BaseImage': 'mcr.microsoft.com/azureml/base-gpu:0.2.0',\n",
       "    'Enabled': True,\n",
       "    'SharedVolumes': True,\n",
       "    'Preparation': None,\n",
       "    'GpuSupport': True,\n",
       "    'ShmSize': '1g',\n",
       "    'Arguments': [],\n",
       "    'BaseImageRegistry': {'Address': None,\n",
       "     'Username': None,\n",
       "     'Password': None}},\n",
       "   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n",
       "    'Packages': [{'Group': 'com.microsoft.ml.spark',\n",
       "      'Artifact': 'mmlspark_2.11',\n",
       "      'Version': '0.12'}],\n",
       "    'PrecachePackages': True}},\n",
       "  'History': {'OutputCollection': True},\n",
       "  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'BatchAi': {'NodeCount': 0},\n",
       "  'AmlCompute': {'Name': None,\n",
       "   'VmSize': None,\n",
       "   'VmPriority': None,\n",
       "   'RetainCluster': False,\n",
       "   'ClusterMaxNodeCount': 1},\n",
       "  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n",
       "  'Mpi': {'ProcessCountPerNode': 1},\n",
       "  'Hdi': {'YarnDeployMode': 2},\n",
       "  'ContainerInstance': {'Region': None, 'CpuCores': 0, 'MemoryGb': 0},\n",
       "  'ExposedPorts': None,\n",
       "  'PrepareEnvironment': None},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://azurenlp7906480779.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-reuters_1548056330117/azureml-logs/60_control_log.txt?sv=2018-03-28&sr=b&sig=FLs%2F2z0YMR2YEbO31FPdxAeyDXmpPiSCmYPkjBTvWAA%3D&st=2019-01-21T07%3A43%3A47Z&se=2019-01-21T15%3A53%3A47Z&sp=r',\n",
       "  'azureml-logs/80_driver_log.txt': 'https://azurenlp7906480779.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-reuters_1548056330117/azureml-logs/80_driver_log.txt?sv=2018-03-28&sr=b&sig=cIc7wvNeg7mk9t6QPdyRobvd%2Fij23pwzpSk0LCfq9Tc%3D&st=2019-01-21T07%3A43%3A47Z&se=2019-01-21T15%3A53%3A47Z&sp=r',\n",
       "  'azureml-logs/azureml.log': 'https://azurenlp7906480779.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-reuters_1548056330117/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=6aecJjpt9990L6trW%2BMlvCa7P6WBBo9oxw30P238yqI%3D&st=2019-01-21T07%3A43%3A47Z&se=2019-01-21T15%3A53%3A47Z&sp=r',\n",
       "  'azureml-logs/55_batchai_execution.txt': 'https://azurenlp7906480779.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-reuters_1548056330117/azureml-logs/55_batchai_execution.txt?sv=2018-03-28&sr=b&sig=b17eVEZRGrF1Re5zYfYYAo7XBAX26RjJvtubuJc%2FEHU%3D&st=2019-01-21T07%3A43%3A47Z&se=2019-01-21T15%3A53%3A47Z&sp=r'}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=False) # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "You now have a model trained on a remote cluster.  Retrieve the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch size': 32, 'epochs': 5, 'accuracy': 0.8045414069456812}\n"
     ]
    }
   ],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see files associated with that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/60_control_log.txt',\n",
       " 'azureml-logs/80_driver_log.txt',\n",
       " 'outputs/reuters_model.h5',\n",
       " 'azureml-logs/azureml.log',\n",
       " 'azureml-logs/55_batchai_execution.txt']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "The last step in the training script wrote the file `outputs/sklearn_mnist_model.pkl` in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n",
    "\n",
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO register model \n",
    "model = run.register_model(model_name='keras_reuters', model_path='outputs/reuters_model.h5')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent hyperparameter tuning (Optional)\n",
    "We have trained the model with one set of hyperparameters, now let's how we can do hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(25, 50, 100),\n",
    "        '--epochs' : choice(1,3,5,10)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new estimator without the above parameters since they will be passed in later. Note we still need to keep the `data-folder` parameter since that's not a hyperparamter we will sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount()\n",
    "}\n",
    "\n",
    "est_hd = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                use_docker = True,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['keras', 'tensorflow-gpu'],\n",
    "                pip_packages = ['azure-cli-core<2.0.55'],\n",
    "                use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termnination policy. The `BanditPolicy` basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric `validation_acc` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveRunConfig(estimator=est_hd, \n",
    "                          hyperparameter_sampling=ps, \n",
    "                          policy=policy, \n",
    "                          primary_metric_name='validation_acc', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=8,\n",
    "                          max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477fdc6c26c647b9a1ab7adb6db06563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSE…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(htr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: keras-reuters_1548058646680\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: keras-reuters_1548058646680\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'keras-reuters_1548058646680',\n",
       " 'target': 'gpucluster',\n",
       " 'status': 'Completed',\n",
       " 'endTimeUtc': '2019-01-21T08:28:39.000Z',\n",
       " 'properties': {'primary_metric_config': '{\"name\": \"validation_acc\", \"goal\": \"maximize\"}',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive'},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://azurenlp7906480779.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-reuters_1548058646680/azureml-logs/hyperdrive.txt?sv=2018-03-28&sr=b&sig=QJ6po04QmWIhMsUaJWpk4TW4IWmtfGzoCm%2BZ9z9N0BA%3D&st=2019-01-21T19%3A47%3A52Z&se=2019-01-22T03%3A57%3A52Z&sp=r'}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htr.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keras-reuters_1548058646680_0': {'validation_acc': [0.7964404881729831], 'batch size': [25], 'epochs': [10], 'accuracy': [0.8045414041326392]}, 'keras-reuters_1548058646680_1': {'validation_acc': [0.8109009944159409], 'batch size': [50], 'epochs': [3], 'accuracy': [0.8098842314811869]}, 'keras-reuters_1548058646680_2': {'validation_acc': [0.8109009944159409], 'batch size': [50], 'epochs': [3], 'accuracy': [0.8103294683585502]}, 'keras-reuters_1548058646680_3': {'validation_acc': [0.8186874333955555], 'batch size': [100], 'epochs': [3], 'accuracy': [0.807212823804531]}, 'keras-reuters_1548058646680_4': {'validation_acc': [0.7853170200370178], 'batch size': [50], 'epochs': [1], 'accuracy': [0.794300974264595]}, 'keras-reuters_1548058646680_5': {'validation_acc': [0.802002226550534], 'batch size': [25], 'epochs': [3], 'accuracy': [0.8049866433984344]}, 'keras-reuters_1548058646680_6': {'validation_acc': [0.7997775272081903], 'batch size': [100], 'epochs': [10], 'accuracy': [0.8045414015849786]}, 'keras-reuters_1548058646680_7': {'validation_acc': [0.8097886550545825], 'batch size': [50], 'epochs': [5], 'accuracy': [0.8094389982660859]}}\n"
     ]
    }
   ],
   "source": [
    "print(htr.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and register best model\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the model files uploaded during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'outputs/reuters_model.h5', 'azureml-logs/azureml.log', 'azureml-logs/55_batchai_execution.txt']\n"
     ]
    }
   ],
   "source": [
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `tf-dnn-mnist` under the workspace for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated, use RunHistoryFacade.assets instead.\n"
     ]
    }
   ],
   "source": [
    "model = best_run.register_model(model_name='keras_reuters_best', model_path='outputs/reuters_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
