{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load workspace and download registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/erimen_cse/AMLProjects/MachineLearningNotebooks/tutorials/config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "os.stat_result(st_mode=33204, st_ino=2097230, st_dev=2081, st_nlink=1, st_uid=1003, st_gid=1003, st_size=61755096, st_atime=1548115720, st_mtime=1548115721, st_ctime=1548115721)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model=Model(ws, 'keras_reuters_best')\n",
    "model.download(target_dir = '.', exist_ok = 'True')\n",
    "import os \n",
    "# verify the downloaded model file\n",
    "os.stat('reuters_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service hosted in ACI. \n",
    "\n",
    "To build the correct environment for ACI, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the ACI\n",
    "* The model you trained before\n",
    "\n",
    "### Create scoring script\n",
    "\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # retreive the path to the model file using the model name\n",
    "    model_path = Model.get_model_path('keras_reuters_best')\n",
    "    model = load_model(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    y_hat = model.predict(data)\n",
    "    # you can return any data type as long as it is JSON-serializable\n",
    "    return json.dumps(y_hat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment file\n",
    "\n",
    "Next, create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs `keras`, `tensorflow` and `azureml-sdk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"keras\")\n",
    "#myenv.add_conda_package(\"tensorflow-gpu\")\n",
    "myenv.add_tensorflow_pip_package(core_type='cpu', version='1.11.0')\n",
    "myenv.add_pip_package(\"azure-cli-core<2.0.55\")\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `myenv.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "    # Required packages for AzureML execution, history, and data preparation.\n",
      "  - azureml-defaults\n",
      "  - tensorflow==1.11.0\n",
      "  - azure-cli-core<2.0.55\n",
      "- keras\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create configuration file\n",
    "\n",
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"Reuters\",  \"method\" : \"keras\"}, \n",
    "                                               description='Predict Reuters with keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in ACI\n",
    "Estimated time to complete: **about 7-8 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Build an image using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file\n",
    "1. Register that image under the workspace. \n",
    "1. Send the image to the ACI container.\n",
    "1. Start up a container in ACI using the image.\n",
    "1. Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Image creation operation finished for image keras-reuters-3:1, operation \"Succeeded\"\n",
      "Creating service\n",
      "Running.......................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "CPU times: user 1.68 s, sys: 42.4 ms, total: 1.72 s\n",
      "Wall time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "# configure the image\n",
    "image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n",
    "                                                  runtime=\"python\", \n",
    "                                                  conda_file=\"myenv.yml\")\n",
    "\n",
    "service = Webservice.deploy_from_model(workspace=ws,\n",
    "                                       name='keras-reuters-3',\n",
    "                                       deployment_config=aciconfig,\n",
    "                                       models=[model],\n",
    "                                       image_config=image_config)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-22T08:05:01,252265861+00:00 - gunicorn/run \n",
      "2019-01-22T08:05:01,256092913+00:00 - rsyslog/run \n",
      "ok: run: rsyslog: (pid 15) 0s\n",
      "2019-01-22T08:05:01,256017410+00:00 - iot-server/run \n",
      "2019-01-22T08:05:01,257855983+00:00 - nginx/run \n",
      "ok: run: rsyslog: (pid 15) 0s\n",
      "ok: run: gunicorn: (pid 13) 0s\n",
      "ok: run: nginx: (pid 14) 0s\n",
      "ok: run: rsyslog: (pid 15) 0s\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2019-01-22T08:05:01,444365578+00:00 - iot-server/finish 1 0\n",
      "2019-01-22T08:05:01,445530424+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "{\"timestamp\": \"2019-01-22T08:05:01.692710Z\", \"message\": \"Starting gunicorn 19.6.0\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.error\", \"msg\": \"Starting gunicorn %s\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:01.693602Z\", \"message\": \"Listening at: http://127.0.0.1:9090 (13)\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.error\", \"msg\": \"Listening at: %s (%s)\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:01.693718Z\", \"message\": \"Using worker: sync\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.error\", \"msg\": \"Using worker: %s\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:01.694360Z\", \"message\": \"worker timeout is set to 300\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.error\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:01.695411Z\", \"message\": \"Booting worker with pid: 43\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.error\", \"msg\": \"Booting worker with pid: %s\", \"stack_info\": null}\n",
      "Initializing logger\n",
      "{\"timestamp\": \"2019-01-22T08:05:06.622478Z\", \"message\": \"{\\\"requestId\\\": \\\"00000000-0000-0000-0000-000000000000\\\", \\\"message\\\": \\\"Starting up app insights client\\\", \\\"apiName\\\": \\\"\\\"}\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/var/azureml-app/aml_logger.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"root\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:06.622766Z\", \"message\": \"{\\\"requestId\\\": \\\"00000000-0000-0000-0000-000000000000\\\", \\\"message\\\": \\\"Starting up request id generator\\\", \\\"apiName\\\": \\\"\\\"}\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/var/azureml-app/aml_logger.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"root\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:06.622903Z\", \"message\": \"{\\\"requestId\\\": \\\"00000000-0000-0000-0000-000000000000\\\", \\\"message\\\": \\\"Starting up app insight hooks\\\", \\\"apiName\\\": \\\"\\\"}\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/var/azureml-app/aml_logger.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"root\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:06.623023Z\", \"message\": \"{\\\"requestId\\\": \\\"00000000-0000-0000-0000-000000000000\\\", \\\"message\\\": \\\"Invoking user's init function\\\", \\\"apiName\\\": \\\"\\\"}\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/var/azureml-app/aml_logger.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"root\", \"stack_info\": null}\n",
      "2019-01-22 08:05:06,623 | azureml.core.run | DEBUG | Could not load run context Failed to load a submitted run, if outside of an execution context, use project.start_run to initialize an azureml.core.Run., switching offline: False\n",
      "2019-01-22 08:05:06,624 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n",
      "2019-01-22 08:05:06,624 | azureml.core.model | DEBUG | RunEnvironmentException: Failed to load a submitted run, if outside of an execution context, use project.start_run to initialize an azureml.core.Run.\n",
      "2019-01-22 08:05:06,624 | azureml.core.model | DEBUG | version is None. Latest version is 1\n",
      "2019-01-22 08:05:06,624 | azureml.core.model | DEBUG | Found model path at azureml-models/keras_reuters_best/1/reuters_model.h5\n",
      "2019-01-22 08:05:06.753822: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Using TensorFlow backend.\n",
      "{\"timestamp\": \"2019-01-22T08:05:07.622130Z\", \"message\": \"{\\\"requestId\\\": \\\"00000000-0000-0000-0000-000000000000\\\", \\\"message\\\": \\\"Users's init has completed successfully\\\", \\\"apiName\\\": \\\"\\\"}\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/var/azureml-app/aml_logger.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"root\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:07.622728Z\", \"message\": \"{\\\"requestId\\\": \\\"00000000-0000-0000-0000-000000000000\\\", \\\"message\\\": \\\"Scoring timeout setting is not found. Use default timeout: 3600000 ms\\\", \\\"apiName\\\": \\\"\\\"}\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/var/azureml-app/aml_logger.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"root\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:16.447871Z\", \"message\": \"127.0.0.1 - - [22/Jan/2019:08:05:16 +0000] \\\"GET / HTTP/1.0\\\" 200 7 \\\"-\\\" \\\"Go-http-client/1.1\\\"\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.access\", \"stack_info\": null}\n",
      "{\"timestamp\": \"2019-01-22T08:05:26.398843Z\", \"message\": \"127.0.0.1 - - [22/Jan/2019:08:05:26 +0000] \\\"GET / HTTP/1.0\\\" 200 7 \\\"-\\\" \\\"Go-http-client/1.1\\\"\", \"host\": \"wk-caas-90accf8280ea4b2cad4721a169732d63-23736b9bf1d664ca708cd5\", \"path\": \"/opt/miniconda/lib/python3.6/site-packages/gunicorn/glogging.py\", \"tags\": \"%(module)s, %(asctime)s, %(levelname)s, %(message)s\", \"level\": \"INFO\", \"logger\": \"gunicorn.access\", \"stack_info\": null}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://13.83.108.51:80/score\n"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test deployed service\n",
    "\n",
    "Earlier you scored all the test data with the local version of the model. Now, you can test the deployed model with a random sample of 30 images from the test data.  \n",
    "\n",
    "The following code goes through these steps:\n",
    "1. Send the data as a JSON array to the web service hosted in ACI. \n",
    "\n",
    "1. Use the SDK's `run` API to invoke the service. You can also make raw calls using any HTTP tool such as curl.\n",
    "\n",
    "1. Print the returned predictions and plot them along with the input images. Red font and inverse image (white on black) is used to highlight the misclassified samples. \n",
    "\n",
    " Since the model accuracy is high, you might have to run the following code a few times before you can see a misclassified sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "num_classes = max(y_train) + 1\n",
    "\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (y_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCORRECT prediction for sample0\n",
      "INCORRECT prediction for sample1\n",
      "correct prediction for sample 2\n",
      "correct prediction for sample 3\n",
      "correct prediction for sample 4\n",
      "correct prediction for sample 5\n",
      "correct prediction for sample 6\n",
      "INCORRECT prediction for sample7\n",
      "correct prediction for sample 8\n",
      "INCORRECT prediction for sample9\n",
      "INCORRECT prediction for sample10\n",
      "correct prediction for sample 11\n",
      "correct prediction for sample 12\n",
      "correct prediction for sample 13\n",
      "INCORRECT prediction for sample14\n",
      "correct prediction for sample 15\n",
      "correct prediction for sample 16\n",
      "correct prediction for sample 17\n",
      "correct prediction for sample 18\n",
      "correct prediction for sample 19\n",
      "correct prediction for sample 20\n",
      "INCORRECT prediction for sample21\n",
      "correct prediction for sample 22\n",
      "correct prediction for sample 23\n",
      "correct prediction for sample 24\n",
      "correct prediction for sample 25\n",
      "correct prediction for sample 26\n",
      "correct prediction for sample 27\n",
      "INCORRECT prediction for sample28\n",
      "correct prediction for sample 29\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# find 30 random samples from test set\n",
    "# n = 30\n",
    "\n",
    "s = 100\n",
    "e = s + 30\n",
    "\n",
    "test_samples = json.dumps({\"data\": x_test[s:e].tolist()})\n",
    "test_samples = bytes(test_samples, encoding = 'utf8')\n",
    "\n",
    "test_labels = y_test[s:e]\n",
    "body = np.array(json.loads(test_samples)['data'])\n",
    "\n",
    "# predict using the deployed model\n",
    "result = json.loads(service.run(input_data=test_samples))\n",
    "\n",
    "for i in range(0, len(result)):\n",
    "    indice_prediction = np.argmax(result[i])\n",
    "    indice_label = np.argmax(test_labels[i])\n",
    "    if indice_prediction == indice_label:\n",
    "        print(\"correct prediction for sample \" + str(i))\n",
    "    else:\n",
    "        print(\"INCORRECT prediction for sample\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n",
      "POST to url http://13.83.108.51:80/score\n",
      "label: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "prediction: \"[[7.148323493311182e-05, 0.0006692331517115235, 2.8482483685365878e-05, 0.9808841943740845, 0.0067803943529725075, 1.5119664567464497e-05, 3.5796758311335e-05, 5.306567618390545e-05, 0.0017548621399328113, 7.204800931503996e-05, 0.00029933039331808686, 0.0009929620428010821, 0.0003102721821051091, 2.1101988750160672e-05, 4.312822420615703e-05, 9.70678956946358e-06, 0.0017854924080893397, 2.1938954887446016e-05, 4.2201299947919324e-05, 0.0025818233843892813, 0.001441393862478435, 0.0004726932675112039, 2.4641873096697964e-05, 0.0001069498248398304, 1.5779825844219886e-05, 3.205460598110221e-05, 7.633689165231772e-06, 3.392386497580446e-05, 3.796141754719429e-05, 9.162038622889668e-05, 0.00013864960055798292, 5.210936797084287e-05, 2.8641175958910026e-05, 2.7729709472623654e-05, 8.61385342432186e-05, 2.5308987460448407e-05, 0.0005800557555630803, 2.3854245227994397e-05, 0.00010192501213168725, 6.226001278264448e-05, 3.370818012626842e-05, 5.2945011702831835e-05, 5.503202828549547e-06, 2.7520634830580093e-05, 7.155788352974923e-06, 9.232035154127516e-06]]\"\n",
      "<class 'str'>\n",
      "[[7.148323493311182e-05, 0.0006692331517115235, 2.8482483685365878e-05, 0.9808841943740845, 0.0067803943529725075, 1.5119664567464497e-05, 3.5796758311335e-05, 5.306567618390545e-05, 0.0017548621399328113, 7.204800931503996e-05, 0.00029933039331808686, 0.0009929620428010821, 0.0003102721821051091, 2.1101988750160672e-05, 4.312822420615703e-05, 9.70678956946358e-06, 0.0017854924080893397, 2.1938954887446016e-05, 4.2201299947919324e-05, 0.0025818233843892813, 0.001441393862478435, 0.0004726932675112039, 2.4641873096697964e-05, 0.0001069498248398304, 1.5779825844219886e-05, 3.205460598110221e-05, 7.633689165231772e-06, 3.392386497580446e-05, 3.796141754719429e-05, 9.162038622889668e-05, 0.00013864960055798292, 5.210936797084287e-05, 2.8641175958910026e-05, 2.7729709472623654e-05, 8.61385342432186e-05, 2.5308987460448407e-05, 0.0005800557555630803, 2.3854245227994397e-05, 0.00010192501213168725, 6.226001278264448e-05, 3.370818012626842e-05, 5.2945011702831835e-05, 5.503202828549547e-06, 2.7520634830580093e-05, 7.155788352974923e-06, 9.232035154127516e-06]]\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(x_test)-1)\n",
    "print (random_index)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(x_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# for AKS deployment you'd need to the service key in the header as well\n",
    "# api_key = service.get_key()\n",
    "# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "# print(\"label:\", y_test[random_index])\n",
    "# print(\"prediction:\", resp.text)\n",
    "\n",
    "import ast\n",
    "result = ast.literal_eval(json.loads(resp.text))\n",
    "#print (result)\n",
    "\n",
    "indice_prediction = np.argmax(result[0])\n",
    "print(indice_prediction)\n",
    "indice_label = np.argmax(y_test[random_index])\n",
    "print(indice_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
